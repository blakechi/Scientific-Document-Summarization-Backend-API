"""
Using Recursively Summarizing Books with Human Feedback as testing context

@article{DBLP:journals/corr/abs-2109-10862,
  author    = {Jeff Wu and
               Long Ouyang and
               Daniel M. Ziegler and
               Nisan Stiennon and
               Ryan Lowe and
               Jan Leike and
               Paul F. Christiano},
  title     = {Recursively Summarizing Books with Human Feedback},
  journal   = {CoRR},
  volume    = {abs/2109.10862},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.10862},
  eprinttype = {arXiv},
  eprint    = {2109.10862},
  timestamp = {Mon, 04 Oct 2021 08:57:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-10862.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
"""


TITLE = "Recursively Summarizing Books with Human Feedback"

ABSTRACT = """
A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases (∼ 5% of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves competitive results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.
"""

SECTIONS = {
    "section 0": """
    To train an ML model on a new task, we need a training signal that tells the model which behaviors are better and which are worse. For some tasks, like playing a video game, this training signal can be calculated automatically. However, for many useful tasks an accurate training signal can only be provided via a human in the loop. For example, humans can provide demonstrations of the correct behavior (Bain and Sammut, 1995) or compare two outputs from the model being trained (Christiano et al., 2017), and this data is used to train the model. In this paper we focus on tasks that are difficult for humans to supervise or evaluate, either because the tasks take a lot of time or because they require specialized knowledge and expertise to evaluate. For example, imagine training a model to summarize an entire sub-field of scientific research. For a human to provide a demonstration or evaluate the quality of a model-generated summary, they would likely need a huge amount of time and expertise. One could circumvent this difficulty by using easier-to-measure proxy objectives (e.g. how often words in the summary relate to the topic, and how accurate individual sentences in the summary are), but these proxies are usually less aligned with our actual goals, and optimizing them can have unintended consequences (Clark and Amodei, 2016; Krakovna et al., 2020; Amodei et al., 2016).
    Successfully training ML systems on such tasks will require more scalable means of producing an effective training signal — this problem is known as scalable oversight (Amodei et al., 2016).
    Our approach to scalable oversight is directly inspired by Christiano et al. (2018) and Leike et al. (2018), who make use of task decomposition (Singh, 1992; Dayan and Hinton, 1993) and learning from human feedback. At a high level, these methods take a top-level task and decompose it into several smaller subtasks whose answers would help a human solve or evaluate the top-level task. These subtasks can in turn be decomposed into smaller tasks until it is feasible for humans to provide a training signal for a leaf task. ML models can be trained to solve the leaf tasks, to solve higher-level tasks given answers to the lower-level tasks, and to decompose the harder tasks into subtasks. While Dayan and Hinton (1993) and Christiano et al. (2018) only tried this on simple algorithmic tasks, Perez et al. (2020) and Rajani et al. (2019) use similar ideas for question-answering tasks using a single step of decomposition. We take a step further in this direction by scaling task decomposition to abstractive book summa- rization. Abstractive book summarization is a difficult task, where dataset collection is challeng- ing (Mihalcea and Ceylan, 2007; Ladhak et al., 2020; Krys ́cin ́ski et al., 2021) and existing methods are typically either extractive (Radev et al., 2004; Mihalcea and Tarau, 2004; Bamman and Smith, 2013; Ladhak et al., 2020) or focused on shorter stories (Kazantseva, 2006; Zhang et al., 2019b). We implement a natural task decomposition for long-form summarization: first, we train models to summarize small parts of the book, and then use these models to help humans summarize larger sections of the book, and continue with this strategy recursively. We train a single model to perform these tasks using standard cross-entropy behavioral cloning (BC) and reinforcement learning (RL) from human preferences (Christiano et al., 2017). Our main result is a model that can be applied recursively to generate plausible summaries of entire books. Our approach lets us summarize books of arbitrary length – we achieve believable summaries on books with hundreds of thousands of words by recursing to depth 3. With a non-recursive approach, generating or evaluating a book summary requires a human reading the entire book, so naively collecting such a dataset is over 50x more expensive per data point (see Appendix E.2). Qualitatively, these summaries contain important events from the book, and sometimes synthesize these details abstractively; however, they often leave out important details or fail to grasp the broader context. When evaluated quantitatively, our model significantly outperforms our behavioral cloning baseline, and a small number of summaries approach human-level quality. Separately, we perform an ablation comparing RL to BC on summarizing smaller sections of a book, and find that RL has better scaling properties. We also evaluate our summaries with the NarrativeQA question-answering dataset (Kocˇisky` et al., 2018) and find that a zero-shot model taking our summaries as input achieves competitive results at answering questions about books and movie scripts. We also achieve state-of- the-art results on the recent BookSum dataset (Krys ́cin ́ski et al., 2021) for book-length summarization. Overall, our results show that combining recursive task decomposition with learning from human feedback can be a practical approach to scalable oversight for difficult long-document NLP tasks. We hope that our work encourages more research in using models trained on simpler tasks to aid humans in providing training signals on more difficult tasks.
    """,
    "section 1": """
    Consider a task for which it is very expensive for a human to provide a training signal. Christiano et al. (2018), Irving et al. (2018), and Leike et al. (2018) all propose in some way reducing the task into simpler parts which humans can supervise. In task decomposition, a human decomposes this parent task into several subtasks, such that each subtask is simpler than the parent task, and having the responses to the subtasks would help a human provide a training signal for the parent task. This task decomposition process can be applied recursively to obtain a tree of tasks, such that the leaf tasks are simple enough for a human to demonstrate or evaluate. For example, the parent task “Write a research report on climate change interventions” might decompose into a subtask like: “Give me a list of the most promising climate change interventions”, which then further decomposes into simpler tasks like “How effective is reducing food waste?” and “What are ways to make nations coordinate in avoiding tragedy of the commons scenarios?”. If we repeat this process many times, we obtain a dataset that we can use to train an ML model. Specifically, given a (sub)task we want to train a model that can perform two fundamental operations: 1. Decompose: Ask for responses to a set of simpler tasks. 2. Respond: Given responses to some number (possibly none) of simpler tasks, respond to the original task. When there are simpler tasks used, we sometimes refer to the operation as Compose, since it composes the sub-responses into an overall response. Then any task can be performed via a recursive procedure if it is amenable to decomposition; we show a pseudocode implementation in Appendix A. It remains an open question to what extent natural tasks are actually amenable to decomposition (Ought, 2020). While the framework above is fully general, it can be further simplified if the task lends itself to a simple recursive structure where the decomposition operation can be performed algorithmically, and the ML model only needs to be trained on the Respond operation.We use a simple procedure to algorithmically decompose a summarization task for a piece of text: If the text is short enough, summarize it directly. If it is longer, chunk the text into smaller pieces, and recursively ask to summarize each one. This results in a tree of summarization tasks (see Figure 1), where only the leaf tasks operate on passages of the original book text.
    Each task, corresponding to nodes with pencil symbols in Figure 1, has a height and depth, which correspond to the standard terminology used for trees. The height of a node is the length of the longest downward path to a leaf from that node. A height 0 task is a leaf task, where the goal is to summarize the original book text. We sometimes refer to tasks that are height > 0 as composition tasks, since the input is a concatenation of summaries, and the goal is to produce another summary. The depth of a node is the length of the path from the node to the root. A depth 0 task is the final summarization task, where the goal is to produce a summary of an entire book (given summaries produced from the depth 1 tasks). An evident issue with the above approach is that tasks corresponding to passages further into a book may lack the necessary context for a successful summary. We remedy this by additionally putting prior summaries in context, from the same depth, concatenated together in order.3 We call these summaries the previous context. In Figure 1, the previous summaries inputs for the blue task are indicated using dotted lines. We include as many prior summaries as can fit in the model’s context length. We would like each summary to flow naturally from the previous context, since it may get concatenated with it at a higher height or in the previous context for a later task. A convenient property of this decomposition is that all of the tasks in the tree are extremely similar to one another. Every task for the model is a summarization task that can be formatted the same way. The input text is either the original book text or a concatenation of summaries, and we optionally have additional previous context in the form of summaries. Pseudocode and detailed parameters of tree construction can be found in Appendix A.5. For training the model, we most closely follow the procedure of Stiennon et al. (2020). We start with a pretrained language model and a pool of trained human labelers (see Appendix B for details). We collect demonstrations from labelers and train a model via behavioral cloning. We then repeat many iterations of reward learning and reinforcement learning. To learn the reward function, we collect comparisons from labelers on outputs from the current best policy and train a reward model to predict log odds that a response is better. Reinforcement learning directly optimizes the reward with an additional KL term to prevent too much drift from the initial policy, typically our best supervised policy. More details in Appendix D. To collect a label for a given task, we need to generate its inputs: if a node is not a leaf, we run an existing model (typically the best available) recursively to generate summaries for each of its children. In summary, we use the following algorithm: 1. Recursively decompose books(and compose child summaries) into tasks using the procedure described in 2.2, using the best models we have4 and the best sampling parameters we have5. While this could be done with humans, it would be prohibitively expensive. 2. Sample a node from the tree, corresponding to a summarization task which we’d like to train on.6 Details below in 2.3.2. 3. Obtain training data, given the inputs to that node (a) For demonstrations, we then have human labelers write a desired output (b) For comparisons, we run the model we wish to train to obtain two outputs, typically at temperature 1. We then ask human labelers to choose which output is better. 4. We then finetune the model using the training data (a) For demonstrations, we use behavior cloning (BC). We do a supervised finetune using the standard cross entropy loss function. (b) For comparisons, we use reinforcement learning (RL) against a reward model trained to predict human preferences. We can iterate this entire process with newer models, different node sampling strategies, and different choice of training data type (demonstration versus comparison). Since each model is trained on inputs produced by a different model, inputs produced by itself are outside of the training distribution, thus causing auto-induced distributional shift (ADS) (Krueger et al., 2020). This effect is more severe at later parts in the tree computation (later in the book, and especially higher in the tree). This means that after each round of training, running the full procedure always results in inputs out of the prior training distributions, for tasks at non-zero height. While we did not systematically measure the severity of this effect, in practice we generally found that additional rounds of training at height 0 resulted in better-rated summaries at height 1. Because of the ADS mentioned in Section 2.3.1, it is advantageous to prioritize training on nodes earlier/lower in the tree computation, before moving to nodes later in the computation. We define the following terms: First subtree. The first subtree refers to the first height 1 task, and its height 0 child tasks (of which there are typically 10-13). See the yellow nodes in Figure 1 for an example. In Section 4.1, we find that by training on merely the first subtree, the model can generalize to the entire tree. First leaves. The first leaves refers to the height 0 tasks in the first subtree, i.e. those which are children of the first height 1 task. For early rounds, we initially train only on the first leaves, since inputs to later nodes depend on having plausible summaries from earlier nodes, and we do not want to use excessive human time. We then move to the entire first subtree (additionally training on a single height 1 task), once the summaries for the first leaves look reasonable. At this point, our model is already capable of generalizing to the full tree, and we switch to training on all nodes. Curriculum changes were made in an ad hoc manner, moving on when we deemed the models "good enough" at earlier tasks. We use pretrained transformer language models (Vaswani et al., 2017) from the GPT-3 family (Brown et al., 2020), which take 2048 tokens of context. Input tokens are produced by the byte pair encoding introduced in Radford et al. (2019). Other architecture and hyperparameters choices follow those of Stiennon et al. (2020). More details in Appendix D.
    Behavioral cloning and reward modeling In the first leaves phase of the project, we collect data for all first leaves together. When moving to first subtree, we independently collect data for the height 1 tasks, letting us vary the ratio of training data at the different heights. Finally, for the full tree phase, we follow a strategy of first randomly sampling a depth, and then randomly selecting a task amongst tasks at that depth. Inputs are typically generated using the best model available and best guess sampling parameters (see Appendix D.2).
    In all cases, we train on all past data (individual demonstrations and comparisons for tasks from various parts of the tree). We then shuffle and sample tasks randomly.
    Reinforcement learning We ran three variants of sampling tasks for reinforcement learning episodes, corresponding to our changes in the training curriculum.
    1. The first leaves: Each episode is a single first leaf task. The algorithm trains on consecutive leaf tasks in succession; the sampled summaries are used as previous context for later leaves.
    2. The first subtree: Each episode consists of a first leaf task or the height 1 composition task for the first subtree. The algorithm trains on the leaf tasks in succession, followed by the composition task using their sampled outputs.
    3. Full tree: We choose a random depth d and then a random node at that depth. The algorithm trains on N successive depth d + 1 tasks followed by a single depth d composition task using those N outputs. Input trees are generated ahead of time from the initial model with best-effort sampling settings (in practice, we sometimes use some trees from older models as well). Since our demonstration and comparison data is at the level of individual nodes, we train the RL policy at the same granularity: each task is its own episode, and no rewards propagate to other nodes of the tree. Compared to end-to-end training, decomposition makes it much easier to collect human feedback for a given task. Correspondingly, it makes the task much easier for the ML model. But it also offers other benefits: 1. It empowers a human to do or evaluate parts of the task themself. For example, a human with access to lower-level summaries can quickly summarize themselves.
    2. It makes it easier to trace what the model is thinking, and debug errors in the model. If a model summary contains a relatively isolated fact, a human with access to the tree can trace it back to the original text. 3. Our procedure generalizes gracefully to longer books. It can be used at test time on books of unbounded length, regardless of the length of books in the training dataset. 
    """,
    "section 2": """
    For training, we use a subset of the books used in GPT-3’s training data (Brown et al., 2020). The books are primarily fiction, and contain over 100K words on average. We further constrain our dataset by asking labelers to skip non-narrative books. We chose narrative fiction books due to our belief that they were the most difficult to summarize, which is supported by our later qualitative findings (Appendix J). Summarizing narrative texts is particularly challenging for extractive methods since any given sentence tends to be a very low-level description. We find additional evidence for this in Section 4.2, where our models outperform an extractive oracle on the BERTScore metric. We aim to summarize abstractively, tracing out narrative arcs and larger themes rather than listing series of events. Our primary metric is labeler judgments of overall summary quality on a 1-7 Likert scale, on held-out books that were neither in the GPT-3 pretraining dataset nor in our book dataset. We also ask labelers to evaluate summary accuracy, coverage of the source text, coherence, and amount of abstraction; see more details on our instructions to labelers in Appendix C.1.
    For each summarization subtask, we generally aim to compress the text by a factor of 5-10x, with length upper limits of 128 to 384 tokens, depending on the task height. We ask labelers to evaluate summary quality conditioned on its length; that is, labelers are answering the question “how good is this summary, given that it is X words long?” This is in part to avoid the scenario where, if longer summaries are preferred by labelers, models will generate the longest summaries allowed by the length constraints (Stiennon et al., 2020). We emphasize that for each subtask, labelers only consider the quality of the summary with respect to the direct input to the model, rather than the subset of the book representing the true summarization target. See Appendix A.3 for more discussion.
    """,
    "section 3": """
    We first evaluate our models’ ability to summarize full books that were unseen during pretraining or fine-tuning. To do this, we use the 40 most popular books published in 2020 according to Goodreads at the time we looked. The resulting books span a variety of genres (see Table 5).
    We then assigned two labelers to read each book (purchased with reimbursement) and to write a summary of the book. Finally, we ask the labelers to rate summaries from various models and from the other labeler. Labeler agreement for relative quality of model-written summaries was nearly 80%. We evaluate two model sizes, 175B parameters and 6B parameters. For each size, we also evaluate three different modes of training: RL on the whole tree, RL on the first subtree, and BC on the whole tree. For each policy, we generate 3 summaries each, in order to reduce error bars. Even for temperature 0 policies, we can vary the summaries by changing the seed used to randomly choose chunking boundaries – we found this to produce significant variation in the summaries. We evaluated all BC policies at temperatures T=0.0, 0.3, and 0.6 on this test set. The results in Figures 2 and 3 use the best temperatures for these policies.7 This is because it was too expensive to ablate temperature on the full book summarization task on our validation set (though we we show temperature sweeps on the validation set for leaf summarization tasks in Appendix D.2, these temperatures are not a priori the best for full book summarization). In the end, we empirically found that the best temperatures for the leaf task were also the best for full book summarization: T=0.6 was best for our 6B BC baseline, and all temperatures performed about equally for our 175B BC baseline. Our best models can generate realistic summaries of books unseen during training. Some of these summaries approach human-level quality: over 5% of summaries from the best 175B model were given a score of 6 out of 7, and over 15% were given a 5 out of 7, scores which were also sometimes assigned to human-written summaries (Figure 3). However, on average our model summaries are still significantly worse than human-written summaries (Figure 2a), See our website8 for our model summaries and ratings.
    We find that training on the first subtree does comparably to training on the full tree (Figure 2b). Our models trained on just the first subtree generalize quite well to the full book summarization task. However, we also found the full tree models disappointing; the final 175B full tree model we trained was noticeably worse than the previous one.9 We discuss possible reasons for this in Appendix G. We also find that our 175B RL policies significantly outperform our 175B BC baseline, though the improvement is smaller for the 6B models. Likert scores for the full book summaries were significantly lower than Likert scores of any of the individual decomposed tasks. This is unsurprising, since the errors accumulated at each depth are all reflected in the full book summary score. See Appendix A.3 for more discussion. We also evaluate our models on the recently proposed BookSum dataset for book-length summariza- tion (Krys ́cin ́ski et al., 2021) We compare to the best extractive (BertExt; Liu and Lapata, 2019b) and abstractive (T5; Raffel et al., 2019) models, as well as an extractive oracle (which uses the reference summary to find the sentences in the source text that lead to the highest score). Krys ́cin ́ski et al. (2021) evaluate book summaries using ROUGE (Lin and Och, 2004), BERTScore (Zhang et al., 2019a), and SummaQA (Scialom et al., 2019). SummaQA requires paragraph-aligned summaries, which we do not have, and so we report results on ROUGE and BERTScore. Our depth 0 summaries are substantially shorter than the reference summaries, so we use the concatenation of depth 1 summaries. Our 175B models beat all non-oracle baselines on ROUGE by 3-4 points and approach the per- formance of an extractive oracle. They also significantly outperform all baselines on BERTScore, including the extractive oracle. The 6B models are comparable to baselines on ROUGE while also significantly outperforming all baselines on BERTScore, including an 11B T5 model (Raffel et al., 2019) fine-tuned on the BookSum dataset. Krys ́cin ́ski et al. (2021) report length being a confounder for BERTScore, with longer summaries having lower scores. We also find a slight negative correlation between length and BERTScore, but controlling for it does not significantly affect our conclusions (see Appendix I). Note that we cannot rule out overlap of the BookSum dataset with our pretraining dataset. Never- theless, from manual inspection of the trees, we believe that the summarization procedure largely reflects the structure of the book, rather than being a result of memorization from pretraining. In Section 4.1.2 we found that our RL models outperformed our BC models. However, our RL models were trained on significantly more data. A significant open question is whether doing RL on summary comparisons is actually better than simple behavior cloning on an equal number of high-quality human demonstrations. Previous results from Stiennon et al. (2020) showed that doing RL greatly improved summary quality over their BC baseline, and even outperformed human-written summaries. However, their reference summaries were scraped from Reddit TL;DRs, which are often not good summaries of the original text, and they do not compare to collecting a similar number of high-quality demonstrations.
    In this work, we use the same trained labelers to create demonstrations and comparisons, and directly compare RL to BC by plotting model performance versus the amount of human time required to produce each dataset. We study this on the first leaf summarization task rather than the full book summarization task to save human time. We trained 3 versions of a 6B parameter BC baseline, with 1⁄4, 1⁄2, and all the demonstrations. Then, we trained RL policies starting from each of the 1⁄4 and 1⁄2 BC policies,10 with approximately the same number of comparisons as there were demonstrations. For these BC policies, we used temperature T=0.6, while for RL policies, we use T=0 (see Appendix D.2 for justification). We found that while RL on comparisons was about as effective as BC on demonstrations after 5k-10k demonstrations, comparisons were far more efficient on the margin after 10k-20k demonstrations (Figure 4). Furthermore, comparisons used to produce this figure were 3x as fast for us to collect as demonstrations (see Appendix E). Another way to evaluate summaries is to test whether they can be used to answer questions about the original text (Scialom et al., 2019; Wang et al., 2020). We applied our summarization model to the NarrativeQA question answering dataset (Kocˇisky` et al., 2018), a dataset consisting of question/answer pairs about full book texts and movie transcripts. The question/answer pairs come from Wikipedia summaries, matched by title to the full text. In the full stories version of NarrativeQA, the model must use the original text.
    We test whether our summaries can be used as input (instead of the full book or movie text) to a question answering (QA) model. For the QA model, we simply use a trained UnifiedQA model (Khashabi et al., 2020) in a zero-shot manner with temperature 0. We can give it either the depth 0 summary, or a concatenation of the depth 1 summaries (the concatenation of depth 2 summaries can be quite long). We found that depth 1 summaries work better.
    As shown in Table 3, we achieve competitive results, despite our summarization model not be- ing trained explicitly for question answering. However, we use far more parameters than Izacard and Grave (2020), the previous SOTA. When using smaller UnifiedQA models for question an- swering, results are substantially worse, suggesting that the quality of the QA model is a primary bottleneck (Figure 7). All our samples are available on our website.
    """,
    "section 4": """
    Our work is directly inspired by previous papers that lay the groundwork for applying human feed- back to reinforcement learning (Christiano et al., 2017), especially to large-scale tasks. Our task decomposition approach can be thought of as a specific instantiation of iterated amplification (Chris- tiano et al., 2018), except we assume a fixed decomposition and start training from the leaf tasks, rather than using the entire tree. Similarly, our approach can be considered a form of recursive reward modeling (Leike et al., 2018) if we understand the purpose of model-generated lower-level summaries to be to help the human evaluate the model’s performance on higher-level summaries. Our contribution over these works is showing that this approach can be realistically applied to a difficult, large-scale task. We also build on the growing body of work that fine-tunes models with human feedback. This has been applied in many domains including summarization (Böhm et al., 2019; Ziegler et al., 2019; Stiennon et al., 2020), dialogue (Jaques et al., 2019; Yi et al., 2019; Hancock et al., 2019), translation (Kreutzer et al., 2018; Bahdanau et al., 2016), semantic parsing (Lawrence and Riezler, 2018), story generation (Zhou and Xu, 2020), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019), and agents in simulated environments (Christiano et al., 2017; Ibarz et al., 2018).
    There has been relatively little work on summarizing novels and other long-form fiction writing. Early work (Gorinski and Lapata, 2015) used graph-based methods to summarize movie scripts. Mihalcea and Ceylan (2007) introduced a dataset of book summaries scraped from CliffsNotes and tested an unsupervised extractive system based on MEAD (Radev et al., 2004) and Textrank (Mihalcea and Tarau, 2004). More recently, Ladhak et al. (2020) propose a method for extractive summarization of chapters of novels. There has been work on generating partial summaries of fictional stories: Zhang et al. (2019b) investigate generating character descriptions written by the story author, and Kazantseva (2006) investigate extractive methods for generating information about the story setting and characters, but not the plot. Relatedly, Bamman and Smith (2013) proposes an unsupervised method for aligning books with human-written summaries. There has also been some work on question answering using full books (Mou et al., 2020; Izacard and Grave, 2020; Zemlyanskiy et al., 2021). Concurrent with our work, Krys ́cin ́ski et al. (2021) extended the datasets of Mihalcea and Ceylan (2007) and evaluated neural baselines.ss
    While work on summarizing novels is sparse, there has been plenty of work on summarizing other kinds of long documents, such as scientific papers (Abu-Jbara and Radev, 2011; Collins et al., 2017; Subramanian et al., 2019; Cohan et al., 2018; Xiao and Carenini, 2019; Zhao et al., 2020; Sotudeh et al., 2020), and patents (Sharma et al., 2019), as well as multi-document summarization (Liu et al., 2018; Ma et al., 2020; Gharebagh et al., 2020; Chandrasekaran et al., 2020; Liu and Lapata, 2019a; Gao et al., 2020). Many of these techniques use a hierarchical approach to generating final summaries, either by having a hierarchical encoder (Cohan et al., 2018; Zhang et al., 2019c; Liu and Lapata, 2019a), or by first running an extractive summarization model followed by an abstractive model (Subramanian et al., 2019; Liu et al., 2018; Zhao et al., 2020; Gharebagh et al., 2020). The latter can be seen as a form of task decomposition, where the leaf task is document- level extractive summarization and the parent task is abstractive summarization conditioned on the extracted summaries.
    The idea of decomposing hard tasks into multiple smaller sub-tasks has been used extensively in NLP. For example, Fan et al. (2018) generate fictional stories by first training models to generate a story prompt, and then training another model to generate the story conditioned on this prompt. The idea of saving human time by using models trained at lower levels of the hierarchy to help humans label data for higher-level tasks has also been explored. In Fan et al. (2020), models are used to search for evidence of facts, to help humans fact check faster and more accurately.
    """,
    "section 5": """
    Our main interest in this work is scaling human feedback to hard problems; we want to empower humans to give feedback to models on tasks that are very difficult to evaluate. We expect this to be a critical part of the alignment problem because we need to make sure humans can communicate their values to AI systems as they take on more societally-relevant tasks (Leike et al., 2018). If we develop techniques to optimize AI systems on what we actually care about, then we make optimization of convenient but misspecified proxy objectives obsolete.
    In this paper, we showed that it is feasible to train models using human feedback on the difficult task of abstractive book summarization, by leveraging task decomposition and learning from human feedback. We also showed that doing RL on summary comparisons is more efficient than supervised learning on summary demonstrations, once the summarization policy has passed a quality threshold. Though we used a fixed decomposition strategy that applies only to summarization, the general techniques could be applied to any task. In this sense we have made progress towards optimizing what we actually care about: good summarization performance as judged by humans. Something we do not address in this paper is training a single model to perform the entire top-level task, e.g. a single model that maps a book to a summary. This could be done via distillation as suggested in Christiano et al. (2018), however in our case that would require training a single model with a very large context window, which introduces additional complexity. Furthermore, since the majority of our compute is at the leaf tasks, this would not save us much compute at test-time. Our model’s book summaries lack coherence. While our models successfully generate book- level summaries that contain much of the important information, they often read more as a list of events from the book, rather than a coherent summary that a human would write. In theory, this could be remedied with more rounds of RL at the top-level summarization task, however in practice we found RL at higher levels of the tree to be challenging (see below).
    Task decomposition could be fundamentally limiting. Task decomposition assumes that separate parts of the task can be completed independently. However, this may not be true for summarizing books. For example, it may be hard to catch cases where earlier details in the book are only later revealed to be important (e.g. in mystery books). Our summarization models also sometimes generate inaccurate statements due to a lack of context; for example, there is a passage of Pride and Prejudice in which the main character gets asked for “their hand”. In the broader context of the chapter, it is clear that the character is being asked for a dance. However, this is not clear from only the local context of the leaf task, and thus the model summarizes it as asking for “her hand in marriage”. This is a general weakness of our training setup because we require each summary to be produced from only this local context, with a model that has not read the rest of the book.
    Some of these issues may be alleviated by learning a decomposition procedure rather than using a fixed algorithm (see Appendix A.3 for some discussion). However, this may not resolve all of the problems with decomposition. Consider a case where important information is sprinkled lightly across many parts of the book, e.g. small details implying a buildup of love or resentment, where each detail is too minor to be included in a chapter summary despite being a prominent overall theme. Determining the kinds of tasks that are amenable to decomposition remains an open problem.
    Training on higher height tasks may be difficult. In general, policy errors at lower levels com- pound at each composition task, ultimately leading to large errors on the top-level task. Auto-induced distributional shift (ADS, see Section 2.3.1) may also be making training significantly more difficult, and curriculum choice may matter a lot as a result. Our curriculum and node sampling strategies were chosen in an ad hoc way. As shown in Section 4.1, training on the full tree of tasks did not lead to improved performance. We discuss some possible reasons in Appendix G but leave thorough investigations to future work. Though our approach produced plausible book summaries, the limitations above suggest some open questions for future research. First, are there better and more principled curricula? Could one obtain improved performance by doing RL more on-policy, by generating the summary trees on the fly, or by training the reward model online as in Ziegler et al. (2019)? Is it better to have longer or shorter episodes, encompassing more or less of the tree? While having longer episodes means the policy has more in-distribution inputs at test time, it also means training on fewer trees for a given amount of compute and makes the reward model less on-distribution.
    There are also many ways to improve the fundamental techniques for fine-tuning models using human feedback. For example, are there more efficient ways to collect data from humans instead of binary comparisons? Could other methods for optimizing against human feedback, such as expert iteration (Anthony et al., 2017), be more efficient? Finally, there are questions for how this procedure extends to other tasks. Is learning a task decompo- sition model, rather than using a fixed decomposition, feasible for hard real-world tasks? For what kinds of tasks is task decomposition fundamentally limiting? How else can we use ML models to assist humans in specifying their preferences for high-level tasks? We hope to address some of these in future work. This work expands on the reward modeling technique proposed in Ziegler et al. (2019) and Stiennon et al. (2020). Thus, the broader impacts are similar to the ones described in those papers. On the positive side, our research is motivated by the benefits of aligning ML systems with human intentions. We believe alignment techniques are an increasingly important tool to improve the safety of ML systems, particularly as these systems become more capable. Conversely, improved alignment could also enable malicious actors to more easily train models that cause harm, and could also lead to increased automation of some jobs, leading to job loss. See the broader impacts discussion of Stiennon et al. (2020) for more discussion of these points. The difference in this paper compared to previous work on reward modeling is that we combine the technique with task decomposition, which allows us to use human feedback to train ML models to perform more difficult tasks. This amplifies both the potential benefits and the risks listed above.
    One point we reiterate from Stiennon et al. (2020) is to be careful when defining the ‘good’ model behavior that labelers will reinforce. In other words, what or who should we align our models to? Deciding what makes a good summary is relatively straightforward, but defining good behavior becomes more difficult as we move beyond summarization to more complex tasks where humans might disagree on the correct model behavior.
    When solely considering the impacts of automatic book summarization, our models still make many mistakes while summarizing, and thus should not be deployed in a setting where high summarization accuracy is necessary. Our model summaries also seek to preserve the intent of the book, whose contents may be harmful or biased.
    """
}